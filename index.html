<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Yiqi Zhu</title>
    <link rel="icon" type="image/x-icon" href="./images/Tsinghua_University_Logo.svg.ico">
    <link rel="stylesheet" href="./assets/css/style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
</head>
<body>
    <main>
        <div class="profile-section">
            <img src="./images/image.jpeg" alt="Yiqi Zhu" class="profile-img">
            <div class="profile-info">
                <h1>Yiqi Zhu <span class="chinese-name">朱奕祺</span></h1>
                <p class="title">Undergraduate Student</p>
                <p class="title">Department of Computer Science and Technology</p>
                <p class="title">Tsinghua University</p>
                <p class="email">zhu-yq22 [at] mails.tsinghua.edu.cn</p>
                <div class="links">
                    <a href="https://x.com/StephenZhu0218" target="_blank"><i class="fab fa-twitter"></i> Twitter</a>
                    <a href="https://scholar.google.com/citations?user=pVSe6agAAAAJ" target="_blank"><i class="fas fa-graduation-cap"></i> Google Scholar</a>
                    <a href="./CV.pdf" target="_blank"><i class="fas fa-file-pdf"></i> CV</a>
                </div>
                <p class="last-updated">Last updated on December 16, 2025</p>
            </div>
        </div>

        <section class="about">
            <h2>About Me</h2>
            <p>Hi there! I am a fourth-year Computer Science Undergraduate Student at Tsinghua University. I enjoy great honor to work with <a href="https://nlp.csai.tsinghua.edu.cn/~ly/" target="_blank">Yang Liu</a> and <a href="https://lpeng.net/" target="_blank">Peng Li</a> on language agents and multimodal large language models since 2023. I also once visited Carnegie Mellon University, proudly working with <a href="https://www.phontron.com" target="_blank">Graham Neubig</a> on coding agents.</p>
            
            <p><span class="research-intro">My research interest broadly lies in artificial intelligence and natural language processing. Currently, I am most interested in the following topics:</span><br>
            1) <em>How to build robust language agents to finish challenging real-world tasks (e.g. solve complicated coding tasks; discover scientific knowledge)?</em><br>
            2) <em>How to empower foundation models with the ability to understand and simulate the real world in various modalities (a.k.a. World Modeling)?</em><br>
            3) <em>How to utilize the generalization ability of (visual) foundation models in embodied scenarios and tasks?</em><br>
            <span class="research-intro">If you are interested in my research or would like to collaborate, feel free to reach out!</span>
            </p>

            <p></p>
        </section>

        <section class="publications">
            <h2>Publications</h2>

            <div class="publication">
                <div class="pub-title">Training Versatile Coding Agents in Synthetic Environments</div>
                <div class="pub-authors"><strong>Yiqi Zhu</strong>, Apurva Gandhi, Graham Neubig</div>
                <div class="pub-venue">Preprint
                    <span class="pub-links">
                        <a href="https://neulab.github.io/SWE-Playground/" target="_blank"><i class="fas fa-home"></i> Homepage</a>
                        <a href="https://github.com/neulab/SWE-Playground" target="_blank"><i class="fab fa-github"></i> Code</a>
                        <a href="https://arxiv.org/abs/2512.12216" target="_blank"><i class="fas fa-file-alt"></i> Paper</a>
                    </span>
                </div>
            </div>
            
            <div class="publication">
                <div class="pub-title">CoSpace: Benchmarking Continuous Space Perception Ability for Vision-Language Models</div>
                <div class="pub-authors"><strong>Yiqi Zhu</strong>*, Ziyue Wang*, Can Zhang, Peng Li, Yang Liu</div>
                <div class="pub-venue">CVPR 2025
                    <span class="pub-links">
                        <a href="https://thunlp-mt.github.io/CoSpace/" target="_blank"><i class="fas fa-home"></i> Homepage</a>
                        <a href="https://github.com/THUNLP-MT/CoSpace" target="_blank"><i class="fab fa-github"></i> Code</a>
                        <a href="https://arxiv.org/abs/2503.14161" target="_blank"><i class="fas fa-file-alt"></i> Paper</a>
                    </span>
                </div>
            </div>
            
            <div class="publication">
                <div class="pub-title">AIGS: Generating Science from AI-Powered Automated Falsification</div>
                <div class="pub-authors">Zijun Liu*, Kaiming Liu*, <strong>Yiqi Zhu</strong>*, Xuanyu Lei*, Zonghan Yang*, Zhenhe Zhang, Peng Li, Yang Liu</div>
                <div class="pub-venue">Tech Report
                    <span class="pub-links">
                        <a href="https://agent-force.github.io/AIGS/" target="_blank"><i class="fas fa-home"></i> Homepage</a>
                        <a href="https://github.com/AgentForceTeamOfficial/Baby-AIGS" target="_blank"><i class="fab fa-github"></i> Code</a>
                        <a href="https://arxiv.org/abs/2411.11910" target="_blank"><i class="fas fa-file-alt"></i> Paper</a>
                    </span>
                </div>
            </div>
            
            <div class="publication">
                <div class="pub-title">Browse and Concentrate: Comprehending Multimodal Content via prior-LLM Context Fusion</div>
                <div class="pub-authors">Ziyue Wang*, Chi Chen*, <strong>Yiqi Zhu</strong>, Fuwen Luo, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Maosong Sun, Yang Liu</div>
                <div class="pub-venue">ACL 2024 Oral
                    <span class="pub-links">
                        <a href="https://thunlp-mt.github.io/Brote/" target="_blank"><i class="fas fa-home"></i> Homepage</a>
                        <a href="https://github.com/THUNLP-MT/Brote" target="_blank"><i class="fab fa-github"></i> Code</a>
                        <a href="https://arxiv.org/abs/2402.12195" target="_blank"><i class="fas fa-file-alt"></i> Paper</a>
                    </span>
                </div>
            </div>
        </section>
    </main>
</body>
</html>
